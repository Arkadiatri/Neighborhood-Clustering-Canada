{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "offensive-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "#import difflib\n",
    "import multiprocess\n",
    "import diff_match_patch as dmp_module\n",
    "\n",
    "DIR_DATA = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-progress",
   "metadata": {},
   "source": [
    "https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "blocked-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMLtoGDF(filename):\n",
    "    gdf = geopandas.read_file(filename)\n",
    "    gdf.rename_geometry('Geometry', inplace=True) # Default geometry column name is 'geometry'; changed for consistent capitalization of columns\n",
    "    gdf.set_geometry('Geometry') # Renaming is insufficient; this sets special variable gdf.geometry = gdf['Geometry']\n",
    "    gdf = gdf.set_crs(epsg=3347) # Needed only for FSA file, the others are 3347 and parsed correctly by geopandas, and the pdf in the zip file has the same projection parameters (FSA vs. DA, ADA, CT)\n",
    "    gdf['Area'] = gdf['Geometry'].to_crs(epsg=6931).area # Equal-area projection\n",
    "    gdf['Centroid'] = gdf['Geometry'].centroid\n",
    "    gdf['Geometry'] = gdf['Geometry'].to_crs(epsg=4326) # Latitude/Longitude representation\n",
    "    gdf['Centroid'] = gdf['Centroid'].to_crs(epsg=4326) # Only the set geometry is converted with gdf.to_crs(); all other geometry-containing columns must be converted explicitly; here we convert all columns explicitly\n",
    "    gdf = gdf.set_crs(epsg=4326) # The series and geodataframe can have separate crs; this was found necessary for the geopandas.union function to operate easily\n",
    "    gdf['Centroid Latitude'] = gdf['Centroid'].geometry.y\n",
    "    gdf['Centroid Longitude'] = gdf['Centroid'].geometry.x\n",
    "    gdf.drop(columns = 'Centroid', inplace=True) # Because WKT Point cannot be serialized to JSON, we drop the Centroid column and keep only its float components\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controversial-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arkadiatri\\anaconda3\\envs\\Coursera\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gdf_CA_FSA = GMLtoGDF(DIR_DATA+'lfsa000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "constitutional-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gdf_CA_DA = GMLtoGDF(DIR_DATA+'lda_000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mental-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gdf1 = gdf_CA_FSA\n",
    "gdf1b = gdf1.copy(deep=True)\n",
    "gdf1b['Geometry'] = gdf1b['Geometry'].buffer(0)\n",
    "gdf2 = gdf_CA_DA\n",
    "gdf2b = gdf2.copy(deep=True)\n",
    "gdf2b['Geometry'] = gdf2b['Geometry'].buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "import testmp\n",
    "import time\n",
    "import ipypb # Lightweight progress bar, source copied from GitHub\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    completed = 0\n",
    "    N = gdf1.shape[0]\n",
    "    results = [None]*N\n",
    "    NUM_PROCESSES = 10\n",
    "    start_time = time.time()\n",
    "    with multiprocess.Pool(NUM_PROCESSES) as pool:\n",
    "        print(f'Generating pool, P={NUM_PROCESSES}, N={N}')\n",
    "        ret = [pool.apply_async(testmp.dmpDiff,(gdf1['Geometry'].iloc[ind], gdf1b['Geometry'].iloc[ind], ind)) for ind in range(N)]\n",
    "        print('Processing pool')\n",
    "        for i in ipypb.track(range(N)): # Alternative: initialize pb and call next(pb) in loop, instead of having a while loop to process all updates since last loop\n",
    "            while True:\n",
    "                indb_finished = [r.ready() for r in ret]\n",
    "                indb_empty = [r==None for r in results]\n",
    "                indb_update = [f and e for f, e in zip(indb_finished, indb_empty)]\n",
    "                if any(indb_update):\n",
    "                    ind_update = indb_update.index(True)\n",
    "                    results[ind_update] = ret[ind_update].get(999999)\n",
    "                    completed += 1\n",
    "                    if completed%1000==0 or completed==1 or completed==N:\n",
    "                        print(f'Finished {completed}/{N}, wall time {time.strftime(\"%H:%M:%S\", time.gmtime(start_time-time.time()))}')\n",
    "                    break\n",
    "            \n",
    "    wall_time = time.time()-start_time\n",
    "    processor_time = sum([sum(r[2]) for r in results])\n",
    "    print(f'Pool processing concluded, process count {sum([r!=None for r in results])}/{N}, wall time {time.strftime(\"%H:%M:%S\", time.gmtime(wall_time))}, processor time {time.strftime(\"%H:%M:%S\", time.gmtime(processor_time))}, speedup {processor_time/wall_time:.3}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-authority",
   "metadata": {},
   "source": [
    "Note that dmp reduction can occur\n",
    "\n",
    "There are no newlines in wkt representation, so we can replace commas with newlines, process, and it will look like the difflib results.  This is necessary because the diff_match_patch module, in line mode, looks for newlines explicitly (does not take delimiters as a parameter)... this might be worth modifiying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "import testmp\n",
    "import time\n",
    "import ipypb # Lightweight progress bar, source copied from GitHub\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    completed = 0\n",
    "    N = gdf1.shape[0]\n",
    "    results = [None]*N\n",
    "    NUM_PROCESSES = 10\n",
    "    start_time = time.time()\n",
    "    with multiprocess.Pool(NUM_PROCESSES) as pool:\n",
    "        print(f'Generating pool, P={NUM_PROCESSES}, N={N}')\n",
    "        ret = [pool.apply_async(testmp.dmpDiffLine,(gdf1['Geometry'].iloc[ind], gdf1b['Geometry'].iloc[ind], ind)) for ind in range(N)]\n",
    "        print('Processing pool')\n",
    "        for i in ipypb.track(range(N)): # Alternative: initialize pb and call next(pb) in loop, instead of having a while loop to process all updates since last loop\n",
    "            while True:\n",
    "                indb_finished = [r.ready() for r in ret]\n",
    "                indb_empty = [r==None for r in results]\n",
    "                indb_update = [f and e for f, e in zip(indb_finished, indb_empty)]\n",
    "                if any(indb_update):\n",
    "                    ind_update = indb_update.index(True)\n",
    "                    results[ind_update] = ret[ind_update].get(999999)\n",
    "                    completed += 1\n",
    "                    break\n",
    "            \n",
    "    wall_time = time.time()-start_time\n",
    "    processor_time = sum([sum(r[2]) for r in results])\n",
    "    print(f'Pool processing concluded, process count {sum([r!=None for r in results])}/{N}, wall time {time.strftime(\"%H:%M:%S\", time.gmtime(wall_time))}, processor time {time.strftime(\"%H:%M:%S\", time.gmtime(processor_time))}, speedup {processor_time/wall_time:.3}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[3][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-compilation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "gentle-thermal",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0c9ab7054857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_wkt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgdf_CA_FSA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "tmp = geopandas.array.to_wkt(gdf_CA_FSA.geometry.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "derived-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = [len(t) for t in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "greenhouse-reduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114640.62283950618, 43054237)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tmp2),max(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "vertical-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp3 = np.sort(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "associate-recipe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43054237,  6468734,  6098821,  5368165,  5049054,  4645296,\n",
       "        4312769,  2720355,  2513920])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp3[-1:-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-bridge",
   "metadata": {},
   "source": [
    "TRY TO MAKE OVERLAP CALCULATION PARALLEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loving-scanner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'testmp' from 'C:\\\\Users\\\\Arkadiatri\\\\AllData\\\\JL-Coursera\\\\9. IBM Applied Data Science Capstone\\\\Coursera_Capstone\\\\testmp.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocess\n",
    "import testmp\n",
    "import time\n",
    "import ipypb # Lightweight progress bar, source copied from GitHub\n",
    "import importlib\n",
    "importlib.reload(testmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "computational-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import dill\n",
    "import gzip\n",
    "DIR_DATA = 'Data/'\n",
    "DIR_RESULTS = 'Results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compatible-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMLtoGDF(filename):\n",
    "    gdf = geopandas.read_file(filename)\n",
    "    gdf.rename_geometry('Geometry', inplace=True) # Default geometry column name is 'geometry'; changed for consistent capitalization of columns\n",
    "    gdf.set_geometry('Geometry') # Renaming is insufficient; this sets special variable gdf.geometry = gdf['Geometry']\n",
    "    gdf = gdf.set_crs(epsg=3347) # Needed only for FSA file, the others are 3347 and parsed correctly by geopandas, and the pdf in the zip file has the same projection parameters (FSA vs. DA, ADA, CT)\n",
    "    gdf['Area'] = gdf['Geometry'].to_crs(epsg=6931).area # Equal-area projection\n",
    "    gdf['Centroid'] = gdf['Geometry'].centroid\n",
    "    gdf['Geometry'] = gdf['Geometry'].to_crs(epsg=4326) # Latitude/Longitude representation\n",
    "    gdf['Centroid'] = gdf['Centroid'].to_crs(epsg=4326) # Only the set geometry is converted with gdf.to_crs(); all other geometry-containing columns must be converted explicitly; here we convert all columns explicitly\n",
    "    gdf = gdf.set_crs(epsg=4326) # The series and geodataframe can have separate crs; this was found necessary for the geopandas.union function to operate easily\n",
    "    gdf['Centroid Latitude'] = gdf['Centroid'].geometry.y\n",
    "    gdf['Centroid Longitude'] = gdf['Centroid'].geometry.x\n",
    "    gdf.drop(columns = 'Centroid', inplace=True) # Because WKT Point cannot be serialized to JSON, we drop the Centroid column and keep only its float components\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-vault",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def loadResults_(name,tuples,fileformat='db',compress=False):\n",
    "    '''Loads variables from files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str, file name base (including directory if desired)\n",
    "    tuples: list of tuples (varname, suffix),\n",
    "        varname: str, the key of the output dict where the data will be stored\n",
    "        suffix: str, the string appended to name to generate a full file name\n",
    "    fileformat: str, suffix to save the file with (do not include period)\n",
    "    compress: bool, True to zip results (appends '.gz' to filename)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None if an error was encountered, or\n",
    "    Tuple the length of tuples containing for each element of tuples:\n",
    "        None if there was an error, or\n",
    "        the variable loaded from file at the same position from tuples\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Files read in binary format with optional gzip encoding\n",
    "    This function is the complement to saveResults_()\n",
    "    \n",
    "    TODO\n",
    "    ----\n",
    "    Add option to change save format (text vs. binary)\n",
    "    Make fileformat select the save format\n",
    "    '''\n",
    "    if type(name)!=str:\n",
    "        print('Error: name must be a string')\n",
    "        return None\n",
    "    if type(fileformat)!=str:\n",
    "        print('Error: fileformat must be a string')\n",
    "        return None\n",
    "    \n",
    "    ret = []\n",
    "    for n, s in tuples:\n",
    "        fn = name+s+'.'+fileformat+('.gz' if compress else '')\n",
    "        try:\n",
    "            with open(fn,'rb') as file:\n",
    "                ret.append(dill.loads(gzip.decompress(file.read()) if compress else file.read()))\n",
    "        except (FileNotFoundError, IOError) as e:\n",
    "            ret.append(None)\n",
    "            print(f'An error was encountered while reading from file {fn}: {e}')\n",
    "    return tuple(ret)\n",
    "\n",
    "def loadResults(name):\n",
    "    '''Loads variables 'gdf_union', 'times', and 'areas' from zipped files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str containing the base name of the files\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None if an error was encountered, or\n",
    "    Tuple the length of tuples containing:\n",
    "        None if there was an error, or\n",
    "        the variable loaded from file at the same position from tuples\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    File names area <name>_<variable>.db.gz and are in gzip dill binary format\n",
    "    Uses outside variable DIR_RESULTS if available, otherwise put path in name\n",
    "    '''\n",
    "    tuples = [('gdf_union',''),\n",
    "              ('times','_times'),\n",
    "              ('areas','_areas')]\n",
    "    \n",
    "    return loadResults_(name,tuples,fileformat='db',compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "saving-plaza",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arkadiatri\\anaconda3\\envs\\Coursera\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n",
      "  for feature in features_lst:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gdf_CA_FSA_D = GMLtoGDF(DIR_DATA+'lfsa000a16g_e.gml')\n",
    "gdf_CA_DA_D = GMLtoGDF(DIR_DATA+'lda_000a16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "younger-violence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gdf_CA_FSA = GMLtoGDF(DIR_DATA+'lfsa000b16g_e.gml')\n",
    "gdf_CA_DA = GMLtoGDF(DIR_DATA+'lda_000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thorough-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gdf1 = gdf_CA_FSA_D\n",
    "key1 = 'CFSAUID'\n",
    "gdf2 = gdf_CA_DA_D\n",
    "key2 = 'DAUID'\n",
    "gdf1b = gdf1.copy(deep=True)\n",
    "gdf1b['Geometry'] = gdf1b['Geometry'].buffer(0)\n",
    "gdf2b = gdf2.copy(deep=True)\n",
    "gdf2b['Geometry'] = gdf2b['Geometry'].buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exterior-lincoln",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loadResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e1aa3e81d2f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDIR_RESULTS\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'GDF_FSA-DA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mareas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loadResults' is not defined"
     ]
    }
   ],
   "source": [
    "r = loadResults(DIR_RESULTS+'GDF_FSA-DA')\n",
    "areas = r[2]\n",
    "times = r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "binary-notion",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f5df302488e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "times = r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "revolutionary-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "normal-selection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1], [0, 4, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[0,.5,1],[2,4,1]]\n",
    "a = [[max(aa) if aaa==max(aa) else 0 for aaa in aa] for aa in a]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pool, P=10, N=56590\n",
      "Processing pool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"566\" value=\"98\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>17%</strong></span>\n",
       "<span class=\"Iteration-label\">95/566</span>\n",
       "<span class=\"Time-label\">[08:47<00:05, 5.38s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [██████████##################################################] 98/566 [08:47<00:05, 5.38s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/566, wall time  0dd00:00:35\n",
      "Finished 10/566, wall time  0dd00:00:55\n",
      "Finished 20/566, wall time  0dd00:01:45\n",
      "Finished 30/566, wall time  0dd00:02:41\n",
      "Finished 40/566, wall time  0dd00:03:36\n",
      "Finished 50/566, wall time  0dd00:04:31\n",
      "Finished 60/566, wall time  0dd00:05:21\n",
      "Finished 70/566, wall time  0dd00:06:06\n",
      "Finished 80/566, wall time  0dd00:06:56\n",
      "Finished 90/566, wall time  0dd00:07:52\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from multiprocessing import get_context\n",
    "\n",
    "    completed = 0\n",
    "    N = gdf2.shape[0]\n",
    "    NUM_PROCESSES = 10\n",
    "    DIV = 100\n",
    "    NDIV = N//DIV+(0 if N%DIV==0 else 1)\n",
    "    results = [None]*NDIV\n",
    "\n",
    "    start_time = time.time()\n",
    "    with multiprocess.Pool(NUM_PROCESSES) as pool: # .get_context(\"spawn\")\n",
    "        print(f'Generating pool, P={NUM_PROCESSES}, N={N}')\n",
    "        ret = [pool.apply_async(testmp.intersectGDFareas,(gdf1,key1,gdf2.iloc[ind*DIV:((ind+1)*DIV if (ind+1)*DIV<N else N),:],key2,areas[ind*DIV:((ind+1)*DIV if (ind+1)*DIV<N else N)],0,6931,gdf1b,gdf2b.iloc[ind*DIV:((ind+1)*DIV if (ind+1)*DIV<N else N),:])) for ind in range(NDIV)]\n",
    "        print('Processing pool')\n",
    "        for i in ipypb.track(range(NDIV)): # Alternative: initialize pb and call next(pb) in loop, instead of having a while loop to process all updates since last loop\n",
    "            while True:\n",
    "                indb_finished = [r.ready() for r in ret]\n",
    "                indb_empty = [r==None for r in results]\n",
    "                indb_update = [f and e for f, e in zip(indb_finished, indb_empty)]\n",
    "                if any(indb_update):\n",
    "                    ind_update = indb_update.index(True)\n",
    "                    results[ind_update] = ret[ind_update].get(999999)\n",
    "                    completed += 1\n",
    "                    if completed%10==0 or completed==1 or completed==(NDIV):\n",
    "                        wall_time = time.time()-start_time\n",
    "                        print(f'Finished {completed}/{NDIV}, wall time {time.gmtime(wall_time).tm_yday - 1}d{time.strftime(\"%H:%M:%S\", time.gmtime(wall_time))}')\n",
    "                    break\n",
    "                time.sleep(5)\n",
    "            \n",
    "    wall_time = time.time()-start_time\n",
    "    processor_time = sum([sum(r[1]) for r in results])\n",
    "    print(f'Pool processing concluded, process count {sum([r!=None for r in results])}/{NDIV}, wall time {time.gmtime(wall_time).tm_yday - 1}d{time.strftime(\"%H:%M:%S\", time.gmtime(wall_time))}, processor time {(processor_time/60/60/24)//1: d}d{time.strftime(\"%H:%M:%S\", time.gmtime(processor_time))}, speedup {processor_time/wall_time:.3}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-medicare",
   "metadata": {},
   "source": [
    "Digital, 10 processes, areas shortcut, buffer pre-processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "import geopandas\n",
    "gdf_union = results[0][0]\n",
    "times = results[0][1]\n",
    "areas = results[0][2]\n",
    "for r in results[1:]:\n",
    "    gdf_union = gdf_union.append(r[0],ignore_index=True)\n",
    "    times.extend(r[1])\n",
    "    areas.extend(r[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-interface",
   "metadata": {},
   "source": [
    "Digital, intersectGDFareas, buffer(0) pre-processed, now removing .get_context(\"spawn\") in pool generation line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-hawaii",
   "metadata": {},
   "source": [
    "Digital, intersectGDFareas, buffer(0) pre-processed\n",
    "\n",
    "    Finished 1/566, wall time 00:00:28\n",
    "    Finished 10/566, wall time 00:00:54\n",
    "    Finished 20/566, wall time 00:01:29\n",
    "    Finished 30/566, wall time 00:02:04\n",
    "    Finished 40/566, wall time 00:02:39\n",
    "    Finished 50/566, wall time 00:03:14\n",
    "    Finished 60/566, wall time 00:03:49\n",
    "    Finished 70/566, wall time 00:04:34\n",
    "    Finished 80/566, wall time 00:05:20\n",
    "    Finished 90/566, wall time 00:05:55\n",
    "    Finished 100/566, wall time 00:06:40\n",
    "    Finished 110/566, wall time 00:07:20\n",
    "    Finished 120/566, wall time 00:08:05\n",
    "    Finished 130/566, wall time 00:08:51\n",
    "    Finished 140/566, wall time 00:09:36\n",
    "    Finished 150/566, wall time 00:10:17\n",
    "    Finished 160/566, wall time 00:11:07\n",
    "    Finished 170/566, wall time 00:11:57\n",
    "    Finished 180/566, wall time 00:12:47\n",
    "    Finished 190/566, wall time 00:13:38\n",
    "    Finished 200/566, wall time 00:14:23\n",
    "    Finished 210/566, wall time 00:15:23\n",
    "    Finished 220/566, wall time 00:16:13\n",
    "    Finished 230/566, wall time 00:16:48\n",
    "    Finished 240/566, wall time 00:17:39\n",
    "    Finished 250/566, wall time 00:18:24\n",
    "    Finished 260/566, wall time 00:19:19\n",
    "    Finished 270/566, wall time 00:20:04\n",
    "    Finished 280/566, wall time 00:20:50\n",
    "    Finished 290/566, wall time 00:21:40\n",
    "    Finished 300/566, wall time 00:22:35\n",
    "    Finished 310/566, wall time 00:23:15\n",
    "    Finished 320/566, wall time 00:24:00\n",
    "    Finished 330/566, wall time 00:24:56\n",
    "    Finished 340/566, wall time 00:25:46\n",
    "    Finished 350/566, wall time 00:26:36\n",
    "    Finished 360/566, wall time 00:27:22\n",
    "    Finished 370/566, wall time 00:28:07\n",
    "    Finished 380/566, wall time 00:28:57\n",
    "    Finished 390/566, wall time 00:29:48\n",
    "    Finished 400/566, wall time 00:30:53\n",
    "    Finished 410/566, wall time 00:32:03\n",
    "    Finished 420/566, wall time 00:32:58\n",
    "    Finished 430/566, wall time 00:33:49\n",
    "    Finished 440/566, wall time 00:34:39\n",
    "    Finished 450/566, wall time 00:35:39\n",
    "    Finished 460/566, wall time 00:36:34\n",
    "    Finished 470/566, wall time 00:37:25\n",
    "    Finished 480/566, wall time 00:38:15\n",
    "    Finished 490/566, wall time 00:39:06\n",
    "    Finished 500/566, wall time 00:39:56\n",
    "    Finished 510/566, wall time 00:40:41\n",
    "    Finished 520/566, wall time 00:41:21\n",
    "    Finished 530/566, wall time 00:42:12\n",
    "    Finished 540/566, wall time 00:42:57\n",
    "    Finished 550/566, wall time 00:43:52\n",
    "    Finished 560/566, wall time 00:44:28\n",
    "    Finished 566/566, wall time 00:44:53\n",
    "    Pool processing concluded, process count 566/566, wall time 00:44:53, processor time 07:06:35, speedup 9.5x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-agency",
   "metadata": {},
   "source": [
    "Cartographic, intersectGDF, buffer(0) pre-processed\n",
    "\n",
    "    565/566 [08:05:30<05:17:42, 51.56s/it]\n",
    "    565/566 [15:04:57<06:59:28, 95.93s/it]\n",
    "\n",
    "    Finished 1/566, wall time 00:01:25\n",
    "    Finished 10/566, wall time 00:04:01\n",
    "    Finished 20/566, wall time 00:06:21\n",
    "    Finished 30/566, wall time 00:08:47\n",
    "    Finished 40/566, wall time 00:11:23\n",
    "    Finished 50/566, wall time 00:13:48\n",
    "    Finished 60/566, wall time 00:16:25\n",
    "    Finished 70/566, wall time 00:18:50\n",
    "    Finished 80/566, wall time 00:21:21\n",
    "    Finished 90/566, wall time 00:23:41\n",
    "    Finished 100/566, wall time 00:26:07\n",
    "    Finished 110/566, wall time 00:28:43\n",
    "    Finished 120/566, wall time 00:31:04\n",
    "    Finished 130/566, wall time 00:33:25\n",
    "    Finished 140/566, wall time 00:35:35\n",
    "    Finished 150/566, wall time 00:38:01\n",
    "    Finished 160/566, wall time 00:40:22\n",
    "    Finished 170/566, wall time 00:42:57\n",
    "    Finished 180/566, wall time 00:45:33\n",
    "    Finished 190/566, wall time 00:48:19\n",
    "    Finished 200/566, wall time 00:50:49\n",
    "    Finished 210/566, wall time 00:53:25\n",
    "    Finished 220/566, wall time 00:55:51\n",
    "    Finished 230/566, wall time 00:58:11\n",
    "    Finished 240/566, wall time 01:00:37\n",
    "    Finished 250/566, wall time 01:03:08\n",
    "    Finished 260/566, wall time 01:05:38\n",
    "    Finished 270/566, wall time 01:08:39\n",
    "    Finished 280/566, wall time 01:11:20\n",
    "    Finished 290/566, wall time 01:14:01\n",
    "    Finished 300/566, wall time 01:16:21\n",
    "    Finished 310/566, wall time 01:18:47\n",
    "    Finished 320/566, wall time 01:21:23\n",
    "    Finished 330/566, wall time 01:23:48\n",
    "    Finished 340/566, wall time 01:26:14\n",
    "    Finished 350/566, wall time 01:29:05\n",
    "    Finished 360/566, wall time 01:32:10\n",
    "    Finished 370/566, wall time 01:35:06\n",
    "    Finished 380/566, wall time 01:37:47\n",
    "    Finished 390/566, wall time 01:40:48\n",
    "    Finished 400/566, wall time 01:43:44\n",
    "    Finished 410/566, wall time 01:46:54\n",
    "    Finished 420/566, wall time 01:50:15\n",
    "    Finished 430/566, wall time 01:53:06\n",
    "    Finished 440/566, wall time 01:56:02\n",
    "    Finished 450/566, wall time 01:58:33\n",
    "    Finished 460/566, wall time 02:01:13\n",
    "    Finished 470/566, wall time 02:03:39\n",
    "    Finished 480/566, wall time 02:06:05\n",
    "    Finished 490/566, wall time 02:08:35\n",
    "    Finished 500/566, wall time 02:11:11\n",
    "    Finished 510/566, wall time 02:13:37\n",
    "    Finished 520/566, wall time 02:16:07\n",
    "    Finished 530/566, wall time 02:18:43\n",
    "    Finished 540/566, wall time 02:21:09\n",
    "    Finished 550/566, wall time 02:23:55\n",
    "    Finished 560/566, wall time 02:28:06\n",
    "    Finished 566/566, wall time 15:04:59\n",
    "    Pool processing concluded, process count 566/566, wall time 15:04:59, processor time 18:42:22, speedup 2.83x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(times)),np.log10(times),'.',alpha=0.2,label='All DAs')\n",
    "plt.legend()\n",
    "plt.xlabel('DA Index')\n",
    "plt.ylabel('Log$_{10}$(Time [s])');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([not r is None for r in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[0:566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "    processor_time = sum([sum(r[1]) for r in results])\n",
    "    print(f'Pool processing concluded, process count {sum([r!=None for r in results])}/{N}, wall time {time.strftime(\"%H:%M:%S\", time.gmtime(wall_time))}, processor time {time.strftime(\"%H:%M:%S\", time.gmtime(processor_time))}, speedup {processor_time/wall_time:.3}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-catering",
   "metadata": {},
   "source": [
    "Digital, 10 process, all overlaps\n",
    "\n",
    "Generating pool, P=10, N=56590\n",
    "    Processing pool\n",
    "\n",
    "    100% 565/566 [01:10:04<00:30, 7.43s/it]\n",
    "\n",
    "    Finished 1/566, wall time 00:00:45\n",
    "    Finished 10/566, wall time 00:01:00\n",
    "    Finished 20/566, wall time 00:02:16\n",
    "    Finished 30/566, wall time 00:03:31\n",
    "    Finished 40/566, wall time 00:04:47\n",
    "    Finished 50/566, wall time 00:05:57\n",
    "    Finished 60/566, wall time 00:07:08\n",
    "    Finished 70/566, wall time 00:08:13\n",
    "    Finished 80/566, wall time 00:09:24\n",
    "    Finished 90/566, wall time 00:10:29\n",
    "    Finished 100/566, wall time 00:11:35\n",
    "    Finished 110/566, wall time 00:12:41\n",
    "    Finished 120/566, wall time 00:13:46\n",
    "    Finished 130/566, wall time 00:14:47\n",
    "    Finished 140/566, wall time 00:15:57\n",
    "    Finished 150/566, wall time 00:17:03\n",
    "    Finished 160/566, wall time 00:18:13\n",
    "    Finished 170/566, wall time 00:19:24\n",
    "    Finished 180/566, wall time 00:20:29\n",
    "    Finished 190/566, wall time 00:21:40\n",
    "    Finished 200/566, wall time 00:22:50\n",
    "    Finished 210/566, wall time 00:24:01\n",
    "    Finished 220/566, wall time 00:25:07\n",
    "    Finished 230/566, wall time 00:26:17\n",
    "    Finished 240/566, wall time 00:27:23\n",
    "    Finished 250/566, wall time 00:28:29\n",
    "    Finished 260/566, wall time 00:29:34\n",
    "    Finished 270/566, wall time 00:30:40\n",
    "    Finished 280/566, wall time 00:31:45\n",
    "    Finished 290/566, wall time 00:33:01\n",
    "    Finished 300/566, wall time 00:34:11\n",
    "    Finished 310/566, wall time 00:35:22\n",
    "    Finished 320/566, wall time 00:36:27\n",
    "    Finished 330/566, wall time 00:37:33\n",
    "    Finished 340/566, wall time 00:38:38\n",
    "    Finished 350/566, wall time 00:39:59\n",
    "    Finished 360/566, wall time 00:41:05\n",
    "    Finished 370/566, wall time 00:42:21\n",
    "    Finished 380/566, wall time 00:43:46\n",
    "    Finished 390/566, wall time 00:44:57\n",
    "    Finished 400/566, wall time 00:46:32\n",
    "    Finished 410/566, wall time 00:48:28\n",
    "    Finished 420/566, wall time 00:50:19\n",
    "    Finished 430/566, wall time 00:52:00\n",
    "    Finished 440/566, wall time 00:53:31\n",
    "    Finished 450/566, wall time 00:54:51\n",
    "    Finished 460/566, wall time 00:56:33\n",
    "    Finished 470/566, wall time 00:57:58\n",
    "    Finished 480/566, wall time 00:59:29\n",
    "    Finished 490/566, wall time 01:00:45\n",
    "    Finished 500/566, wall time 01:02:00\n",
    "    Finished 510/566, wall time 01:03:11\n",
    "    Finished 520/566, wall time 01:04:16\n",
    "    Finished 530/566, wall time 01:05:27\n",
    "    Finished 540/566, wall time 01:06:32\n",
    "    Finished 550/566, wall time 01:07:58\n",
    "    Finished 560/566, wall time 01:09:19\n",
    "    Pool processing concluded, process count 566/56590, wall time 01:10:04, processor time 11:09:04, speedup 9.55x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-victorian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "auburn-heavy",
   "metadata": {},
   "source": [
    "    diff1_i1 : All lines, was previously diff1_instance\n",
    "    diff1_i2 : Only diff lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff1_i1 = [[[('- ' if i<0 else '+ ' if i>0 else '  ')+ss for ss in s.split(',') if ss!=''] for i, s in r[0]] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_i1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff1_i2 = [[[('- ' if i<0 else '+ ' if i>0 else '  ')+ss for ss in s.split(',') if ss!=''] for i, s in r[0] if i!=0] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_i2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff1_1 = [[] for d in diff1_i1]\n",
    "_ = [[l.extend(dd) for dd in d if dd!=''] for d, l in zip(diff1_i1, diff1_1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff1_2 = [[] for d in diff1_i2]\n",
    "_ = [[l.extend(dd) for dd in d if dd!=''] for d, l in zip(diff1_i2, diff1_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_2[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-democracy",
   "metadata": {},
   "source": [
    "Now check the lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len_diff1_1 = [sum(len(dd) for dd in d) for d in diff1_1]\n",
    "len_diff1_2 = [sum(len(dd) for dd in d) for d in diff1_2]\n",
    "all([a>=b for a, b in zip(len_diff1_1,len_diff1_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "[*zip(len_diff1_1,len_diff1_2)][809]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diff1_i1[809][0]),len(diff1_i1[809][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-spare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(len_diff1_1), np.argmax(len_diff1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sort(len_diff1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-arabic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff1_instance_divided = [[[(i, ss) for ss in s.split(',')] for i, s in r[0]] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_instance_divided[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[for i, s in r[0]] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_instance[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_i1_donly[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_diff1_instance = [sum(len(dd) for dd in d) for d in diff1_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_diff1_instance[809]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_diff1_i1= [sum(len(dd) for dd in d) for d in diff1_i1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_diff1_i1[809]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_diff1_i1_donly = [sum(len(dd) for dd in d) for d in diff1_i1_donly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_diff1_i1_donly[809]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y[:len(t)],np.sqrt(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-elevation",
   "metadata": {},
   "source": [
    "Ahah, the time is quadratic in the output length..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_i1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diff1_instance[3]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_len = [sum(1 for _ in d) for d in diff1_i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_instance = diff1_i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of line alterations for each geometry pair\n",
    "diff1_changecount = [sum(1 for e in d if e[0]!=' ') for d in diff1_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_changenumber = sum(np.array(diff1_changecount)>0)\n",
    "buffind_diff1 = np.nonzero(diff1_changecount)[0]\n",
    "print(f'There are {diff1_changenumber} FSA geometries with alterations upon buffering')\n",
    "print('Indices of altered FSA geometries:')\n",
    "print(buffind_diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(np.where(match1==False)[0] == np.where(np.array(diff1_changecount)>0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "minind = 0 # Index to display, sorted by ascending number of diff lines\n",
    "ind_changed = np.where(np.array(diff1_changecount)>0)[0]\n",
    "ind_changed_minlen = ind_changed[np.argsort(np.array(diff1_len)[ind_changed])[minind]]\n",
    "print(f\"The {1+minind}{'st' if 1+minind==1 else 'nd' if 1+minind==2 else 'rd' if 1+minind==3 else 'th'} shortest geometry that changes with buffering is at index {ind_changed_minlen} with length of {diff1_len[ind_changed_minlen]} coordinates (including modification listings)\\n\")\n",
    "display(diff1_instance[ind_changed_minlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condenseDiff(diff):\n",
    "    '''Extracts changed lines from diff and cancels equivalent additions and subtractions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    diff: a list of strings, the result of a line difference (difflib.ndiff())\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of strings, the lines of diff comprising changes that do not appear as both additions and subtractions\n",
    "    '''\n",
    "    addlist = set(d[2:] for d in diff if d[0]=='+')\n",
    "    sublist = set(d[2:] for d in diff if d[0]=='-')\n",
    "    addlist_short = addlist.difference(sublist)\n",
    "    sublist_short = sublist.difference(addlist)\n",
    "    addlist_short = ['+ '+a for a in addlist_short]\n",
    "    sublist_short = ['- '+s for s in sublist_short]\n",
    "    return [*addlist_short, *sublist_short]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "condenseDiff(diff1_instance[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condenseDiffPoints(diff, keep_comments=False, strip_test=False, strip_output=False, use_sets=False):\n",
    "    '''Extracts changed lines from diff and cancels equivalent additions and subtractions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    diff: list of str, the result of a line difference (difflib.ndiff()) on a WKT geometry that was split by comma delimiters\n",
    "    keep_comments: bool, if True, comments prefixed by '?' and their associated line are kept even if the associated line's point could be paired and cancelled\n",
    "    strip_test: bool, if True, \n",
    "    strip_output: bool, if True, returns the bare points prefixed with addition and subtraction instead of the original diff line\n",
    "    use_sets: bool, if True, perform cancelling using sets instead of lists (may not preserve point repetition)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of strings, the lines of diff comprising changes that do not appear as both additions and subtractions\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    1) Create stripped lists for cancel testing\n",
    "    '''\n",
    "    diff = np.array(diff.copy())\n",
    "    addind = list(np.where([d[0]=='+' for d in diff])[0])\n",
    "    subind = list(np.where([d[0]=='-' for d in diff])[0])\n",
    "    altind = list(np.where([d[0]=='?' for d in diff])[0]) # if difflib.ndiff() functions properly, '?' only follows '+' or '-'\n",
    "    \n",
    "    if keep_comments==False:\n",
    "        altind = []\n",
    "        addind_alt = []\n",
    "        subind_alt = []\n",
    "        addind_lon = addind\n",
    "        subind_lon = subind\n",
    "    else:\n",
    "        addind_alt = [i for i in addind if (i+1) in altind]\n",
    "        subind_alt = [i for i in subind if (i+1) in altind]\n",
    "        addind_lon = [i for i in addind if not (i in addind_alt)]\n",
    "        subind_lon = [i for i in subind if not (i in subind_alt)]\n",
    "\n",
    "    # Get only the points for comparison\n",
    "    difflist = np.array([d[2:] for d in diff])\n",
    "    if strip_test:\n",
    "        difflist = np.array([d[(d.rfind(\"(\")+1):(len(d) if d.find(\")\")==-1 else d.find(\")\"))] for d in difflist])\n",
    "    addlist = difflist[addind]\n",
    "    sublist = difflist[subind]\n",
    "\n",
    "    if use_sets:\n",
    "        addlist = set(addlist)\n",
    "        sublist = set(sublist)\n",
    "        addlist_short = addlist.difference(sublist)\n",
    "        sublist_short = sublist.difference(addlist)\n",
    "        addind_short = [np.where(difflist==r)[0][0] for r in addlist_short]\n",
    "        subind_short = [np.where(difflist==r)[0][0] for r in sublist_short]\n",
    "        strip_output = True\n",
    "\n",
    "    else:\n",
    "        addlist_short = []\n",
    "        addind_short = []\n",
    "        sublist_short = sublist.copy()\n",
    "        sublist_short = list(np.array(sublist_short)[[list(subind).index(i) for i in subind_lon]])\n",
    "        subind_short = subind_lon.copy()\n",
    "\n",
    "        for ind, val in zip(addind, addlist):\n",
    "            if ind in addind_lon:\n",
    "                if val in sublist_short:\n",
    "                    sind = sublist_short.index(val)\n",
    "                    sublist_short.remove(val)\n",
    "                    subind_short.remove(subind_short[sind])\n",
    "                else:\n",
    "                    addlist_short.append(val)\n",
    "                    addind_short.append(ind)\n",
    "    \n",
    "    ret_ind = addind_alt + subind_alt + altind + addind_short + subind_short\n",
    "    ret_ind.sort()\n",
    "    \n",
    "    if strip_output:\n",
    "        retlist  = list(difflist[ret_ind])\n",
    "        retlist = [('+ ' if (d in addind_alt + addind_short) else '- ' if (d in subind_alt+subind_short) else '? ') + r.strip() for d, r in zip(ret_ind,retlist)]\n",
    "    else:\n",
    "        retlist = list(diff[ret_ind])\n",
    "    return retlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "condenseDiffPoints(diff1_instance[3], keep_comments=True, strip_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1_instance[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "x = [] # Condensed diff1_instance\n",
    "t = [] # computation time\n",
    "for i in range(len(diff1_instance)):\n",
    "    start_time = time.time()\n",
    "    x.append(condenseDiffPoints(diff1_instance[i], keep_comments=False, strip_test=True))\n",
    "    end_time = time.time()\n",
    "    t.append(end_time-start_time)\n",
    "    print(f'{i}, {t[-1]:.6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-individual",
   "metadata": {},
   "source": [
    "Had to stop on iteration 809, kept going forever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [sum(len(dd) for dd in d) for d in diff1_instance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y[:len(t)],np.sqrt(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-baptist",
   "metadata": {},
   "source": [
    "Ahah, the time is quadratic in the input length... makes sense as I have many loops?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [sum(len(dd) for dd in d) for d in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "plt.scatter([sum([len(yy) for yy in y]) for y in x],t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff1_changelist = [condenseDiffPoints(d, keep_comments=False, strip_test=True, strip_output=False) for d in diff1_instance]\n",
    "diff1_changelist_ind = [i for i, d in enumerate(diff1_changelist) if d!=[]]\n",
    "diff1_changelist_ind_added = [sum(1 for e in diff1_changelist[i] if e[0]=='+') - sum(1 for e in diff1_changelist[i] if e[0]=='-') for i in diff1_changelist_ind]\n",
    "buffind_changed1 = np.array(diff1_changelist_ind)\n",
    "buffind_added1 = buffind_changed1[np.nonzero(diff1_changelist_ind_added)[0]]\n",
    "print(f'There are {len(diff1_changelist_ind)} FSA geometries with non-condensable alterations.')\n",
    "print(f\"There are {len(buffind_added1)} FSA geometries with altered total number of points.\\n\")\n",
    "for i, j in zip(diff1_changelist_ind, diff1_changelist_ind_added):\n",
    "    print(f\"Altered FSA points at index {i}, with total {len(diff1_instance[i])} diff lines, net change {j} coordinates:\")\n",
    "    display(diff1_changelist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-israeli",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-dominant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-bullet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-moses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'asdf,saf'\n",
    "print(s)\n",
    "s = s.replace(',','\\n')\n",
    "print(s)\n",
    "s = s.replace('\\n',',')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_match_patch\n",
    "help(diff_match_patch.diff_match_patch.diff_cleanupSemantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_match_patch\n",
    "help(diff_match_patch.diff_match_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "wkt = ''\n",
    "wkt.replace(',','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all([r!=None for r in results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    " |  BLANKLINEEND = re.compile('\\\\n\\\\r?\\\\n$')\n",
    " |  \n",
    " |  BLANKLINESTART = re.compile('^\\\\r?\\\\n\\\\r?\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(diff_match_patch.diff_match_patch.diff_linesToChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_match_patch\n",
    "#help(diff_match_patch.diff_match_patch.diff_linesToChars)\n",
    "help(diff_match_patch.diff_match_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(diff_match_patch.diff_match_patch.diff_linesToChars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(diff_match_patch.diff_match_patch.diff_bisect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ipypb.progressbar.ConfigurableProgressBar.__next__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "indb_empty = [r==None for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(r[0]) for r in results].index(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[809]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "import testmp # Within notebooks, the function executed in a new process must be in a separate file, or else it will execute only within the notebook process\n",
    "import time\n",
    "import ipypb # Lightweight progress bar, source copied from GitHub\n",
    "\n",
    "completed = 0\n",
    "start_time = time.time()\n",
    "if __name__ == '__main__':\n",
    "    completed = 0\n",
    "    N = gdf1.shape[0]\n",
    "    results = [None]*N\n",
    "    NUM_PROCESSES = 10\n",
    "    with multiprocess.Pool(NUM_PROCESSES) as pool:\n",
    "        print(f'Generating pool, P={NUM_PROCESSES}, N={N}')\n",
    "        ret = [pool.apply_async(testmp.dmpDiff,(gdf1['Geometry'].iloc[ind], gdf1b['Geometry'].iloc[ind], ind)) for ind in range(N)]\n",
    "        print('Processing pool')\n",
    "        while True:\n",
    "            indb_finished = [r.ready() for r in ret]\n",
    "            indb_empty = [r==None for r in results]\n",
    "            indb_update = [f and e for f, e in zip(indb_finished, indb_empty)]\n",
    "            for i, b in enumerate(indb_update):\n",
    "                if not b: continue\n",
    "                results[i] = ret[i].get(999999)\n",
    "                completed += 1\n",
    "                #print(f'completed {completed: >4}/{N}, index {i: >4} in {time.strftime(\"%H:%M:%S\", time.gmtime(sum(results[i][2])))}; total processor time {time.strftime(\"%H:%M:%S\", time.gmtime(sum([sum(r[2]) for r in results if r!=None])))}, total wall time {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "            if all(indb_finished):\n",
    "                break\n",
    "        #print(f'Processing complete in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "    wall_time = time.time()-start_time\n",
    "    processor_time = sum([sum(r[2]) for r in results])\n",
    "    print(f'Pool processing concluded, wall time {time.strftime(\"%H:%M:%S\", time.gmtime(wall_time))}, processor time {time.strftime(\"%H:%M:%S\", time.gmtime(processor_time))}, speedup {processor_time/wall_time:.3}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "indb_nochange = [len(r[0])==1 for r in results]\n",
    "ind_change = [i for i, b in enumerate(indb_nochange) if (not b)]\n",
    "print(ind_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocess.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(dmp_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-aerospace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
